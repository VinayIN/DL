{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.Forget Gate Layer: </b> $f_{t} = \\sigma (W_{f} . [h_{t-1},x_{t}] + b_{f})$\n",
    "\n",
    "<b>2.Input Gate Layer: </b> $i_{t} = \\sigma (W_{i} . [h_{t-1},x_{t}] + b_{i})$\n",
    "\n",
    "<b>3.Candidate value Layer: </b> $\\widetilde{C}_{t} = a_{t} = g_{t} = tanh(W_{c} . [h_{t-1},x_{t}] + b_{c})$\n",
    "\n",
    "<b>4.Output Gate Layer: </b> $O_{t} = \\sigma (W_{o} . [h_{t-1},x_{t}] + b_{o})$\n",
    "\n",
    "\n",
    "<b>New State: $C_{t} = f_{t} \\ast C_{t-1} + i_{t-1} \\ast \\widetilde{C}_{t-1}$ </b><br>\n",
    "<b>Output at t: </b> $h_{t} = tanh(c_{t})\\ast O_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions to be used\n",
    "<b>input: </b>x $\\in$ (5,4)<br>\n",
    "$h_{t} \\in$ (5,1)<br>\n",
    "$[x,h_{t}] \\in$ (5,5)<br>\n",
    "\n",
    "$w_{f},w_{i},w_{c},w_{o} \\in$ (5,5)\n",
    "\n",
    "$b_{f},b_{i},b_{c},b_{o} \\in$ (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Single sentence is taken for example with 5 words and having and embedding dimension of 4. each word is fed to the lstm cell in an unrolled rnn cell against another translated foreign word having a dimention of 1. \n",
    "\n",
    "|  input| word  \t|     Embedding Dimension 4  \t    \t    \t     \t\n",
    "|:------|:----------|:-----:|:-----:|:-----:|:-----:| \n",
    "| x[0] \t| possibly \t| 1.2 \t| 0.2 \t| 5.1 \t| 1.3 \t|\n",
    "| x[1] \t| i        \t| 0.2 \t| 1.1 \t| 3.1 \t| 1.9 \t|\n",
    "| x[2] \t| shall    \t| 5.0 \t| 2.1 \t| 1.2 \t| 5.1 \t|\n",
    "| x[3] \t| come     \t| 3.0 \t| 1.4 \t| 2.7 \t| 1.6 \t|\n",
    "| x[4] \t| tommorow \t| 2.3 \t| 1.4 \t| 3.6 \t| 6.4 \t|\n",
    "\n",
    "|      \t| target \t|     \t|\n",
    "|------\t|--------\t|-----\t|\n",
    "| h[0] \t| awe    \t| 1.2 \t|\n",
    "| h[1] \t| su     \t| 0.2 \t|\n",
    "| h[2] \t| inh    \t| 5.0 \t|\n",
    "| h[3] \t| meiyh  \t| 3.0 \t|\n",
    "| h[4] \t| niyah  \t| 2.3 \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### If you need to use this comment out every line in below cell ######\n",
    "\n",
    "# #### Data Loading ###\n",
    "# #### X=[x,h(t)] ##### dim=(10,6) #####\n",
    "# def get_data():\n",
    "#     X=np.zeros(shape=(5,5),dtype=np.float32)\n",
    "#     print(\"4 float value as input =========> 1 float value as label\")\n",
    "#     for i in range(len(X[:,0])):\n",
    "#         for j in range(len(X[0,:])):\n",
    "#             X[i,j]=float(input())\n",
    "#     print(\"[x,h(t)]: \",X.shape)\n",
    "#     print(\"=======>Random weight value\")\n",
    "#     w_f=np.random.rand(1,5)\n",
    "#     b_f=np.random.rand(1,1)\n",
    "    \n",
    "#     w_i=np.random.rand(1,5)\n",
    "#     b_i=np.random.rand(1,1)\n",
    "    \n",
    "#     w_c=np.random.rand(1,5)\n",
    "#     b_c=np.random.rand(1,1)\n",
    "    \n",
    "#     w_o=np.random.rand(1,5)\n",
    "#     b_o=np.random.rand(1,1)\n",
    "    \n",
    "#     print(\"w_f: \",w_f.shape,\"w_i:\",w_i.shape,\"w_c:\",w_c.shape,\"w_o:\",w_o.shape)\n",
    "#     W=dict()\n",
    "#     b=dict()\n",
    "#     W={\"w_f\":w_f,\n",
    "#       \"w_i\":w_i,\n",
    "#       \"w_c\":w_c,\n",
    "#       \"w_o\":w_o}\n",
    "#     B={\"b_f\":b_f,\n",
    "#       \"b_i\":b_i,\n",
    "#       \"b_c\":b_c,\n",
    "#       \"b_o\":b_0}\n",
    "#     return X,W,B\n",
    "\n",
    "# X,W,B=get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you need to use this comment out each line in above cell #######\n",
    "\n",
    "\n",
    "#### Testing values #####\n",
    "X=[[1,2,0.5],\n",
    "  [0.5,3,1.25]]\n",
    "X=np.array(X,dtype=np.float32)\n",
    "\n",
    "w_c=[[0.45,0.25,0.15]]\n",
    "b_c=[[0.2]]\n",
    "w_c=np.array(w_c,dtype=np.float32)\n",
    "b_c=np.array(b_c,dtype=np.float32)\n",
    "\n",
    "w_i=[[0.95,0.8,0.8]]\n",
    "b_i=[[0.65]]\n",
    "w_i=np.array(w_i,dtype=np.float32)\n",
    "b_i=np.array(b_i,dtype=np.float32)\n",
    "\n",
    "w_f=[[0.7,0.45,0.1]]\n",
    "b_f=[[0.15]]\n",
    "w_f=np.array(w_f,dtype=np.float32)\n",
    "b_f=np.array(b_f,dtype=np.float32)\n",
    "\n",
    "w_o=[[0.6,0.4,0.25]]\n",
    "b_o=[[0.1]]\n",
    "w_o=np.array(w_o,dtype=np.float32)\n",
    "b_o=np.array(b_o,dtype=np.float32)\n",
    "\n",
    "W=dict()\n",
    "B=dict()\n",
    "W={\"w_f\":w_f,\n",
    "  \"w_i\":w_i,\n",
    "  \"w_o\":w_o,\n",
    "  \"w_c\":w_c}\n",
    "B={\"b_f\":b_f,\n",
    "  \"b_i\":b_i,\n",
    "  \"b_o\":b_o,\n",
    "  \"b_c\":b_c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### defined globally ####\n",
    "dim_embed=2\n",
    "s=np.zeros(shape=(dim_embed+1,1),dtype=np.float32)\n",
    "h=np.zeros(shape=(dim_embed+1,1),dtype=np.float32)\n",
    "f=np.zeros(shape=(dim_embed+1,1),dtype=np.float32)\n",
    "i=np.zeros(shape=(dim_embed+1,1),dtype=np.float32)\n",
    "o=np.zeros(shape=(dim_embed+1,1),dtype=np.float32)\n",
    "a=np.zeros(shape=(dim_embed+1,1),dtype=np.float32)\n",
    "## h[-1]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(sample_X,W,B):\n",
    "    print(\"Forward-PASS: %d\"%t)\n",
    "    f[t]=sigmoid(np.matmul(W[\"w_f\"],np.concatenate((sample_X,h[t-1]),axis=0).reshape(dim_embed+1,-1))+B[\"b_f\"])\n",
    "    i[t]=sigmoid(np.matmul(W[\"w_i\"],np.concatenate((sample_X,h[t-1]),axis=0).reshape(dim_embed+1,-1))+B[\"b_i\"])\n",
    "    o[t]=sigmoid(np.matmul(W[\"w_o\"],np.concatenate((sample_X,h[t-1]),axis=0).reshape(dim_embed+1,-1))+B[\"b_o\"])\n",
    "    a[t]=np.tanh(np.matmul(W[\"w_c\"],np.concatenate((sample_X,h[t-1]),axis=0).reshape(dim_embed+1,-1))+B[\"b_c\"])\n",
    "    \n",
    "    #save the state\n",
    "    s[t]=np.multiply(f[t],s[t-1])+np.multiply(i[t],a[t])\n",
    "    #save the output\n",
    "    h[t]=np.multiply(np.tanh(s[t]),o[t])\n",
    "    \n",
    "    print(f[t],i[t],o[t],a[t],s[t],h[t])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. ],\n",
       "       [0.5, 3. ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward-PASS: 0\n",
      "[0.85195273] [0.96083426] [0.8175745] [0.8177541] [0.78572613] [0.5363134]\n",
      "Forward-PASS: 1\n",
      "[0.87030196] [0.981184] [0.8499334] [0.84980404] [1.5176332] [0.7719812]\n"
     ]
    }
   ],
   "source": [
    "### start the forward pass for the sentence with 5 words #####\n",
    "t=0\n",
    "for k in range(len(X[:,-1])):\n",
    "    t=k\n",
    "    forward_pass(X[t,0:-1],W,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 2.  , 0.5 ],\n",
       "       [0.5 , 3.  , 1.25]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulas used: <br>\n",
    "$\\delta h_{t} = \\Delta_{t} + \\Delta_{h_{t}}$ <br>\n",
    "$\\Delta_{h_{t}}=U^{T}.\\delta_{gate}$\n",
    "\n",
    "where,<br>\n",
    "$\\Delta_{t} = \\Delta$ error at time t ===> ($O-\\widehat{O}$)<br>\n",
    "$\\Delta_{h_{t}}$ = error propagated from next step(t+1)<br>\n",
    "\n",
    "$\\delta s_{t} = \\delta h_{t}\\odot o_{t}\\odot(1 - tanh^{2}(s_{t})) + \\delta s_{t+1} \\odot f_{t+1} $<br>\n",
    "$\\delta a_{t} = \\delta s_{t} \\odot i_{t} \\odot (1 - a_{t}^{2})$<br>\n",
    "$\\delta i_{t} = \\delta s_{t} \\odot a_{t} \\odot i_{t} \\odot (1 - i_{t})$<br>\n",
    "$\\delta f_{t} = \\delta s_{t} \\odot s_{t-1} \\odot f_{t} \\odot (1 - f_{t})$<br>\n",
    "$\\delta o_{t} = \\delta h_{t} \\odot tanh(s_{t}) \\odot o_{t} \\odot (1 - o_{t})$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(sample_X,DELTA_t,DELTA_ht,delta_stt):\n",
    "    print(\"Backward Pass:%d\"%t)\n",
    "    delta_ht = DELTA_t + DELTA_ht\n",
    "    delta_st = np.multiply(np.multiply(delta_ht,o[t]),(1-np.tanh(np.tanh(s[t])))) + np.multiply(delta_stt,f[t+1])\n",
    "    delta_at = np.multiply(np.multiply(delta_st,i[t]),(1-np.power(a[t],2)))\n",
    "    delta_it = np.multiply(np.multiply(np.multiply(delta_st,a[t]),i[t]),(1-i[t]))\n",
    "    delta_ft = np.multiply(np.multiply(np.multiply(delta_st,s[t-1]),f[t]),(1-f[t]))\n",
    "    delta_ot = np.multiply(np.multiply(np.multiply(delta_ht,np.tanh(s[t])),o[t]),(1-o[t-1]))\n",
    "    \n",
    "    #using approximation\n",
    "    delta_zt=np.array([delta_at, delta_it, delta_ft, delta_ot]).reshape(4,-1)\n",
    "    \n",
    "    #wt row sequence at,it,ft,ot\n",
    "    ### column_major_[zt]*row_major_[X]\n",
    "    delta_wt=np.matmul(delta_zt,sample_X.reshape(1,-1))\n",
    "    delta_B=delta_zt\n",
    "    \n",
    "    ##update DELTA_ht to be propagated to next state\n",
    "    DELTA_ht=np.matmul(U,delta_zt)\n",
    "    return delta_wt,delta_B,delta_st,DELTA_ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset X\n",
      "[[1.  2. ]\n",
      " [0.5 3. ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input dataset X\")\n",
    "print(X[:,0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formula to calculate change in weight / bias value w.r.t error\n",
    "\n",
    "$f_{t} = \\sigma (W_{f} . [h_{t-1},x_{t}] + b_{f})$<br>\n",
    "$i_{t} = \\sigma (W_{i} . [h_{t-1},x_{t}] + b_{i})$<br>\n",
    "$a_{t} = tanh(W_{c} . [h_{t-1},x_{t}] + b_{c})$<br>\n",
    "$O_{t} = \\sigma (W_{o} . [h_{t-1},x_{t}] + b_{o})$<br>\n",
    "<hr>\n",
    "$z_{t} = [ a_{t},i_{t},f_{t},o_{t} ] = W \\ast [X,h[t-1]] + B $ ............(1)<br> \n",
    "<hr>\n",
    "\n",
    "Derivating the above equation for $\\delta w \\; \\& \\; \\delta b \\; using \\; \\delta z$\n",
    "\n",
    "$\\delta z_{t} = [ \\delta a_{t},\\delta i_{t}, \\delta f_{t}, \\delta o_{t} ]^{T}$ <br>\n",
    "\n",
    "$\\delta w_{t} = \\delta z_{t} \\ast [X,h[t-1]]_{t}$<br>\n",
    "$\\delta b_{t} = \\delta z_{t} $<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start the backward pass from last word to first word and calculate 𝛿w,𝛿b wrt 𝛿error ######\n",
    "## Total(𝛿w,𝛿b) \n",
    "total_delta_wt=np.zeros(shape=(4,dim_embed+1),dtype=np.float32)\n",
    "total_delta_B=np.zeros(shape=(4,1),dtype=np.float32)\n",
    "\n",
    "## Weight matrix for pred\n",
    "U=np.array([W[\"w_c\"][:,-1], W[\"w_i\"][:,-1], W[\"w_f\"][:,-1], W[\"w_o\"][:,-1]]).reshape(1,-1)\n",
    "\n",
    "## Original labels\n",
    "label=np.array(X[:,-1]).reshape(dim_embed,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.5363134 0.7719812 0.       ]]\n",
      "Actual:  [[0.5  1.25]]\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted: \",h.reshape(1,-1))\n",
    "print(\"Actual: \",label.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass:1\n",
      "Backward Pass:0\n"
     ]
    }
   ],
   "source": [
    "delta_stt=0.0 #next state change\n",
    "DELTA_ht=0.0 #Initial output change\n",
    "while t>=0:\n",
    "    sample_X=np.concatenate((X[t,0:-1],h[t-1]),axis=0)\n",
    "    \n",
    "    ## 𝛿error=pred-original\n",
    "    DELTA_t=h[t] - label[t]\n",
    "    \n",
    "    delta_wt,delta_B,delta_stt,DELTA_ht=backward_pass(sample_X,DELTA_t,DELTA_ht,delta_stt)\n",
    "    total_delta_wt+=delta_wt\n",
    "    total_delta_B+=delta_B\n",
    "    t=t-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04559817 -0.15315111 -0.01661359]\n",
      " [-0.00380239 -0.01117041 -0.00095615]\n",
      " [-0.00503912 -0.03023473 -0.0054051 ]\n",
      " [-0.02700757 -0.188653   -0.03610405]]\n"
     ]
    }
   ],
   "source": [
    "print(total_delta_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06108686]\n",
      " [-0.0046938 ]\n",
      " [-0.01007824]\n",
      " [-0.06066704]]\n"
     ]
    }
   ],
   "source": [
    "print(total_delta_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weight based on SGD function with learning rate λ = 0.01\n",
    "$W^{new}=W - \\lambda \\ast W^{total\\_delta\\_wt}$<br>\n",
    "$B^{new}=B - \\lambda \\ast B^{total\\_delta\\_B}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "def update_parameter(matA,matB):\n",
    "    return matA - (learning_rate * matB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================Old values=======================\n",
      "weights=======\n",
      "[[0.45 0.25 0.15]\n",
      " [0.95 0.8  0.8 ]\n",
      " [0.7  0.45 0.1 ]\n",
      " [0.6  0.4  0.25]]\n",
      "bias==========\n",
      "[[0.2 ]\n",
      " [0.65]\n",
      " [0.15]\n",
      " [0.1 ]]\n"
     ]
    }
   ],
   "source": [
    "### Dont change the order of formation of array\n",
    "weight=np.array([W[\"w_c\"],W[\"w_i\"],W[\"w_f\"],W[\"w_o\"]]).reshape(4,-1)\n",
    "bias=np.array([B[\"b_c\"],B[\"b_i\"],B[\"b_f\"],B[\"b_o\"]]).reshape(4,-1)\n",
    "print(\"=====================Old values=======================\")\n",
    "print(\"weights=======\")\n",
    "print(weight)\n",
    "print(\"bias==========\")\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=update_parameter(weight,total_delta_wt)\n",
    "bias=update_parameter(bias,total_delta_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================new values=======================\n",
      "weights=======\n",
      "[[0.4500456  0.25015315 0.15001662]\n",
      " [0.9500038  0.80001116 0.80000097]\n",
      " [0.70000505 0.45003024 0.1000054 ]\n",
      " [0.600027   0.40018865 0.2500361 ]]\n",
      "bias==========\n",
      "[[0.20006108]\n",
      " [0.6500047 ]\n",
      " [0.15001008]\n",
      " [0.10006067]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=====================new values=======================\")\n",
    "print(\"weights=======\")\n",
    "print(weight)\n",
    "print(\"bias==========\")\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "[Understanding](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) backpropagation through time.\n",
    "\n",
    "Reference to a [Numerical Example.](https://medium.com/@aidangomez/let-s-do-this-f9b699de31d9) \n",
    "\n",
    "Mathematical expression used for derivations. [Slides](http://arunmallya.github.io/writeups/nn/lstm/index.html#/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
